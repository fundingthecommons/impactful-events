# Funding the Commons Residency Vetting Protocol

## Overview

This document outlines the comprehensive evaluation system for assessing applications to the FTC Residency program. The system uses structured scoring across multiple criteria to ensure consistent, fair, and thorough candidate evaluation.

## Evaluation Framework

### Core Principles
- **Multi-stage Review**: Progressive filtering through 5 distinct stages
- **Weighted Scoring**: Different criteria have different importance levels
- **Multiple Reviewers**: Consensus-based decision making
- **Evidence-Based**: Decisions backed by specific reasoning and scores

### Scoring Distribution
- **Technical Criteria (30%)**
  - Technical Skills Assessment (12%)
  - Open Source & Collaborative Experience (10%)
  - Learning & Adaptation Ability (8%)

- **Project Criteria (25%)**
  - Project Vision & Feasibility (10%)
  - Public Goods & RealFi Alignment (8%)
  - Impact Potential & Measurement Understanding (7%)

- **Community Fit Criteria (25%)**
  - Public Goods Ecosystem Understanding (8%)
  - Community Contribution Potential (9%)
  - Commitment & Availability (8%)

- **Video Assessment (20%)**
  - Communication Skills (8%)
  - Passion & Authenticity (7%)
  - Professionalism & Presentation (5%)

## Review Process Stages

### 1. Initial Screening (SCREENING)
**Purpose**: Quick qualification check and basic fit assessment  
**Duration**: 10-15 minutes per application  
**Focus**: Eliminate clearly unqualified candidates

#### Screening Checklist:
- [ ] Application is complete (all required fields filled)
- [ ] Meets basic eligibility criteria (availability, visa status)
- [ ] Has relevant technical background
- [ ] Project idea is within scope of residency
- [ ] No obvious red flags (spam, completely off-topic, etc.)

#### Quick Score Guide:
- **8-10**: Excellent fit, strong candidate
- **6-7**: Good fit, solid candidate  
- **4-5**: Marginal fit, needs more evaluation
- **1-3**: Poor fit, likely to reject

### 2. Detailed Review (DETAILED_REVIEW)
**Purpose**: In-depth evaluation of technical and project criteria  
**Duration**: 30-45 minutes per application  
**Focus**: Thorough assessment of technical capability and project viability

#### Technical Skills Assessment (12% weight)
**What to evaluate:**
- Self-reported skill levels and their credibility
- Portfolio/GitHub evidence quality
- Depth vs breadth of technical expertise
- Alignment with residency technical needs

**Scoring Guide:**
- **9-10**: World-class expertise, extensive portfolio, perfect fit for residency needs
- **7-8**: Strong skills, good portfolio, well-suited for program
- **5-6**: Adequate skills, some evidence, can contribute meaningfully  
- **3-4**: Basic skills, limited evidence, may struggle with technical aspects
- **1-2**: Minimal skills, no compelling evidence, poor technical fit

**Evidence to look for:**
- GitHub repositories with meaningful contributions
- Open source project involvement
- Technical depth in their answers
- Realistic self-assessment of skill level

#### Open Source & Collaborative Experience (10% weight)
**What to evaluate:**
- Quality and impact of open source contributions
- Ability to work in distributed, collaborative environments
- Evidence of community engagement

**Scoring Guide:**
- **9-10**: Significant open source contributions, clear collaborative leadership
- **7-8**: Regular contributions, good collaborative skills demonstrated
- **5-6**: Some contributions, basic collaborative experience
- **3-4**: Limited contributions, unclear collaborative abilities
- **1-2**: No meaningful open source experience

#### Project Vision & Feasibility (10% weight)
**What to evaluate:**
- Clarity and innovation of proposed project
- Realistic scope for 3-week timeframe
- Technical feasibility given their skills
- Potential for meaningful completion

**Scoring Guide:**
- **9-10**: Exceptional project idea, perfectly scoped, high innovation
- **7-8**: Strong project idea, well-scoped, clear value proposition
- **5-6**: Good project idea, reasonable scope, solid execution potential
- **3-4**: Unclear project idea, poorly scoped, execution concerns
- **1-2**: Vague or unrealistic project, major feasibility issues

### 3. Video Review (VIDEO_REVIEW)
**Purpose**: Assess communication skills, passion, and presentation quality  
**Duration**: 15-20 minutes per application (including video watch time)  
**Focus**: Personal qualities and communication effectiveness

#### Communication Skills (8% weight)
**What to evaluate:**
- Clarity of expression and articulation
- English proficiency and comprehension
- Ability to explain complex concepts simply
- Overall communication effectiveness

**Scoring Guide:**
- **9-10**: Exceptional communicator, crystal clear, engaging delivery
- **7-8**: Strong communicator, clear and professional
- **5-6**: Adequate communication, gets points across effectively
- **3-4**: Basic communication, some clarity issues
- **1-2**: Poor communication, difficult to understand

#### Passion & Authenticity (7% weight)
**What to evaluate:**
- Genuine enthusiasm for the mission
- Authentic personal connection to the work
- Energy level and engagement
- Intrinsic vs. extrinsic motivation

**Scoring Guide:**
- **9-10**: Exceptional passion, deeply authentic, infectious enthusiasm
- **7-8**: Strong passion, clearly authentic, genuine motivation
- **5-6**: Good enthusiasm, seems genuine, solid motivation
- **3-4**: Limited passion, somewhat scripted, unclear motivation
- **1-2**: No passion evident, feels artificial, opportunistic

#### Video Assessment Guidelines:
- **Technical Quality**: Don't penalize for basic equipment, but assess preparation
- **Time Management**: Did they use the full minute effectively?
- **Content Structure**: Clear introduction, key points, appropriate conclusion
- **Body Language**: Confident, engaged, professional presentation

### 4. Consensus Building (CONSENSUS)
**Purpose**: Team discussion and alignment on final decisions  
**Duration**: 15-30 minutes per application (in group setting)  
**Focus**: Reconciling different reviewer perspectives

#### Consensus Process:
1. **Score Review**: Compare individual reviewer scores
2. **Discuss Discrepancies**: Focus on applications with wide score variations
3. **Share Key Insights**: Each reviewer shares most important observations
4. **Build Agreement**: Work toward consensus recommendation
5. **Document Rationale**: Record final reasoning for decision

#### Handling Disagreements:
- **Major Score Differences (>3 points)**: Mandatory discussion
- **Recommendation Conflicts**: Additional senior reviewer input
- **Tie Situations**: Err on side of giving candidates the benefit of doubt

### 5. Final Decision (FINAL_DECISION)
**Purpose**: Final accept/reject/waitlist determinations  
**Duration**: 5-10 minutes per application  
**Focus**: Converting consensus into actionable decisions

#### Decision Categories:
- **ACCEPT**: Clear admit, strong fit across all criteria
- **WAITLIST**: Good candidate, decision depends on pool strength
- **REJECT**: Not a good fit for current program

## Detailed Scoring Rubrics

### Public Goods Ecosystem Understanding (8% weight)
**Question Context**: "What does 'funding the commons' mean to you?", funding mechanisms experience, etc.

**Scoring Guide:**
- **9-10**: Deep, nuanced understanding; familiar with multiple mechanisms (QF, retroPGF, etc.); thoughtful critique
- **7-8**: Solid understanding; knows key concepts; some mechanism familiarity
- **5-6**: Basic understanding; surface-level knowledge; generic responses
- **3-4**: Limited understanding; buzzword usage without substance
- **1-2**: No meaningful understanding; completely off-base responses

### Community Contribution Potential (9% weight)
**Question Context**: "What can you offer to others in the cohort?", connections sought, etc.

**Scoring Guide:**
- **9-10**: Unique, valuable skills; strong mentorship ability; clear collaborative mindset
- **7-8**: Good skills to share; some mentorship potential; collaborative attitude
- **5-6**: Standard contribution; willing to help; basic collaborative skills
- **3-4**: Limited contribution; unclear collaborative skills; mostly seeking help
- **1-2**: No clear contribution; individualistic; may be detrimental to cohort

### Impact Measurement Understanding (7% weight)
**Question Context**: "How do you think we should measure the impact of public goods?"

**Scoring Guide:**
- **9-10**: Sophisticated understanding; multiple measurement frameworks; aware of challenges
- **7-8**: Good understanding; several measurement approaches; realistic expectations
- **5-6**: Basic understanding; some measurement ideas; general awareness
- **3-4**: Limited understanding; simplistic measurement approach; unrealistic expectations
- **1-2**: No meaningful understanding; no coherent measurement approach

## Quality Assurance & Calibration

### Calibration Sessions
**Frequency**: Before each major evaluation cycle  
**Duration**: 2-3 hours  
**Purpose**: Ensure consistent scoring across reviewers

#### Calibration Process:
1. **Sample Evaluation**: All reviewers evaluate 3-5 sample applications
2. **Compare Scores**: Identify areas of significant disagreement
3. **Discuss Rationale**: Share reasoning behind scores
4. **Refine Understanding**: Clarify criteria interpretation
5. **Re-evaluate**: Score samples again to check alignment

### Common Scoring Pitfalls
- **Halo Effect**: High score in one area influencing others
- **Anchoring**: First application setting reference point for all others
- **Similarity Bias**: Favoring candidates similar to yourself
- **Recency Effect**: Later applications scored differently than earlier ones
- **Grade Inflation**: Scores creeping upward over time

### Score Distribution Guidelines
**Expected distribution across candidate pool:**
- **9-10 (Exceptional)**: ~5-10% of applicants
- **7-8 (Strong)**: ~15-25% of applicants  
- **5-6 (Adequate)**: ~40-50% of applicants
- **3-4 (Weak)**: ~15-25% of applicants
- **1-2 (Poor)**: ~5-10% of applicants

## Operational Guidelines

### Review Assignments
- **Screening**: 1 reviewer per application
- **Detailed Review**: 2 reviewers per application
- **Video Review**: 1-2 reviewers per application (can be combined with detailed)
- **Consensus**: All reviewers participate

### Time Management
- **Daily Quotas**: Max 8-10 detailed reviews per reviewer per day
- **Break Frequency**: 15-minute break every 2 hours
- **Review Sessions**: 2-3 hour focused sessions work best
- **Avoid Rush**: Don't compress timeline at expense of quality

### Documentation Requirements
- **Score Reasoning**: Brief explanation for any score <4 or >8
- **Key Observations**: 2-3 bullet points of notable strengths/concerns
- **Video Notes**: Specific observations about communication, passion, presentation
- **Consensus Notes**: Summary of discussion and decision rationale

## System Usage Guide

### Accessing the Review System
1. Navigate to `/admin/events/[eventId]/applications`
2. Click "Review Pipeline" tab
3. Select applications from appropriate stage column
4. Complete evaluation form with scores and comments

### Evaluation Form Best Practices
- **Score First**: Give initial scores, then refine based on reasoning
- **Use Reasoning Fields**: Always explain scores, especially extreme ones
- **Video Timestamps**: Note specific moments that influenced your assessment
- **Save Frequently**: Use "Save Progress" to avoid losing work
- **Complete Fully**: Don't submit incomplete evaluations

### Pipeline Management
- **Stage Progression**: Applications automatically move between stages
- **Assignment Tracking**: Monitor your assigned reviews in dashboard
- **Deadline Awareness**: Complete reviews by assigned due dates
- **Status Updates**: Keep team informed of progress and blockers

## Decision Framework Examples

### Accept Decision Profile
```
Technical: 7-9 (Strong skills, clear evidence)
Project: 7-9 (Compelling, feasible project)
Community: 7-9 (Great fit, valuable contributor)
Video: 6-9 (Adequate to excellent communication)
Overall: 7.0+ average
```

### Waitlist Decision Profile
```
Technical: 6-8 (Good skills, some concerns)
Project: 5-8 (Interesting project, scope questions)
Community: 6-8 (Good fit, standard contribution)
Video: 5-8 (Adequate to good communication)
Overall: 5.5-7.0 average
```

### Reject Decision Profile
```
Technical: 1-6 (Insufficient skills for program)
Project: 1-6 (Unclear, unfeasible, or off-topic)
Community: 1-6 (Poor fit, limited contribution)
Video: 1-6 (Communication concerns)
Overall: <5.5 average OR critical weakness in key area
```

## Continuous Improvement

### Post-Cycle Review
After each cohort selection:
1. **Decision Accuracy**: Compare predicted vs. actual performance
2. **Process Efficiency**: Identify bottlenecks and improvements
3. **Criteria Relevance**: Assess which criteria best predicted success
4. **Scorer Agreement**: Analyze inter-rater reliability
5. **Update Protocol**: Revise guidelines based on learnings

### Feedback Integration
- **Cohort Performance**: How did selected candidates perform?
- **Diversity Goals**: Did process support diversity objectives?
- **Time Efficiency**: Can process be streamlined without quality loss?
- **Scorer Experience**: What challenges did reviewers face?

---

*This protocol should be reviewed and updated after each cohort cycle to incorporate learnings and improve accuracy.*